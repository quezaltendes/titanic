import os
import io
import re
import json
import pickle
import torch
from PIL import Image
import torchvision.transforms as T
from torchvision.transforms.functional import InterpolationMode
from transformers import Qwen3VLForConditionalGeneration, AutoProcessor

# =============================================================================
# üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∏
# =============================================================================
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
MODEL_PATH = "./Qwen3-VL-8B-Instruct-FP8"

# =============================================================================
# üîπ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
# =============================================================================
processor = AutoProcessor.from_pretrained(MODEL_PATH, trust_remote_code=True)
model = Qwen3VLForConditionalGeneration.from_pretrained(
    MODEL_PATH,
    torch_dtype=torch.float16 if DEVICE == "cuda" else torch.float32,
    device_map="auto",
    trust_remote_code=True
)
model.eval()

# =============================================================================
# üîπ –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (–∏–∑ vllm)
# =============================================================================
IMAGENET_MEAN = (0.485, 0.456, 0.406)
IMAGENET_STD = (0.229, 0.224, 0.225)

def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):
    best_ratio_diff = float('inf')
    best_ratio = (1, 1)
    area = width * height
    for ratio in target_ratios:
        target_aspect_ratio = ratio[0] / ratio[1]
        ratio_diff = abs(aspect_ratio - target_aspect_ratio)
        if ratio_diff < best_ratio_diff:
            best_ratio_diff = ratio_diff
            best_ratio = ratio
        elif ratio_diff == best_ratio_diff:
            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:
                best_ratio = ratio
    return best_ratio

def dynamic_preprocess(image, min_num=1, max_num=12, image_size=1536, use_thumbnail=False):
    orig_width, orig_height = image.size
    aspect_ratio = orig_width / orig_height
    target_ratios = sorted(
        {(i, j) for n in range(min_num, max_num + 1)
         for i in range(1, n + 1) for j in range(1, n + 1)
         if min_num <= i * j <= max_num},
        key=lambda x: x[0] * x[1]
    )
    target_aspect_ratio = find_closest_aspect_ratio(
        aspect_ratio, target_ratios, orig_width, orig_height, image_size
    )
    target_width = image_size * target_aspect_ratio[0]
    target_height = image_size * target_aspect_ratio[1]
    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]
    resized_img = image.resize((int(target_width), int(target_height)))
    processed_images = []
    for i in range(blocks):
        box = (
            (i % (target_width // image_size)) * image_size,
            (i // (target_width // image_size)) * image_size,
            ((i % (target_width // image_size)) + 1) * image_size,
            ((i // (target_width // image_size)) + 1) * image_size
        )
        split_img = resized_img.crop(box)
        processed_images.append(split_img)
    if use_thumbnail and len(processed_images) != 1:
        thumbnail_img = image.resize((image_size, image_size))
        processed_images.append(thumbnail_img)
    return processed_images

# =============================================================================
# üîπ –ò–Ω—Ñ–µ—Ä–µ–Ω—Å
# =============================================================================


def extract_answer_from_response(response: str) -> str:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ—Ç–≤–µ—Ç –∏–∑ –ø–æ—Å–ª–µ–¥–Ω–µ–π —Å—Ç—Ä–æ–∫–∏ –º–æ–¥–µ–ª–∏:
    - –µ—Å–ª–∏ –µ—Å—Ç—å –¥–≤–æ–µ—Ç–æ—á–∏–µ ‚Äî –±–µ—Ä—ë—Ç –≤—Å—ë –ø–æ—Å–ª–µ –Ω–µ–≥–æ (–≤ –≤–µ—Ä—Ö–Ω–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ);
    - –µ—Å–ª–∏ –Ω–µ—Ç ‚Äî –±–µ—Ä—ë—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–π —ç–ª–µ–º–µ–Ω—Ç –ø–æ—Å–ª–µ split().
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–ª—å–∫–æ –±—É–∫–≤—ã A‚ÄìF (—É–Ω–∏–∫–∞–ª—å–Ω—ã–µ, –≤ –ø–æ—Ä—è–¥–∫–µ –ø–æ—è–≤–ª–µ–Ω–∏—è).
    –ï—Å–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –æ–∂–∏–¥–∞–µ–º—ã–º –≤–∞—Ä–∏–∞–Ω—Ç–∞–º (A, B, C, D, AB, AC, –∏ —Ç.–¥.), –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç "A".
    """
    if not response:
        return "A"  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º A –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è –ø—É—Å—Ç–æ–≥–æ –æ—Ç–≤–µ—Ç–∞

    last_line = response.strip().splitlines()[-1].strip()

    if ':' in last_line:
        part = last_line.split(':', 1)[1].strip().upper()
    else:
        part = last_line.split()[-1].strip().upper() if last_line.split() else ""

    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –±—É–∫–≤—ã A-F
    matches = re.findall(r'[A-F]', part)
    if matches:
        seen = set()
        result = ''.join([m for m in matches if not (m in seen or seen.add(m))])
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ–∂–∏–¥–∞–µ–º—ã–º –≤–∞—Ä–∏–∞–Ω—Ç–∞–º
        if is_valid_answer(result):
            return result
        else:
            return "A"
    
    return "A"  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º A, –µ—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –±—É–∫–≤

def is_valid_answer(answer: str) -> bool:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –æ—Ç–≤–µ—Ç –¥–æ–ø—É—Å—Ç–∏–º—ã–º:
    - –î–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Ç–æ–ª—å–∫–æ –±—É–∫–≤—ã A, B, C, D (–º–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä–∏—Ç—å –¥–æ F –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏)
    - –î–ª–∏–Ω–∞ –æ—Ç 1 –¥–æ 2 —Å–∏–º–≤–æ–ª–æ–≤ (–º–æ–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å)
    - –ë—É–∫–≤—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º–∏
    """
    if not answer or len(answer) > 2:  # –ú–∞–∫—Å–∏–º—É–º 2 –±—É–∫–≤—ã
        return False
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤—Å–µ —Å–∏–º–≤–æ–ª—ã –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ A-D
    if not all(char in 'ABCD' for char in answer):
        return False
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å —Å–∏–º–≤–æ–ª–æ–≤ (—É–∂–µ –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–æ –≤ extract_answer_from_response)
    return len(set(answer)) == len(answer)


def infer_one(model, processor, image_bytes, question_text):
    """–ò–Ω—Ñ–µ—Ä–µ–Ω—Å –æ–¥–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞ (–æ–¥–∏–Ω —Ä–∏—Å—É–Ω–æ–∫ –∏ –≤–æ–ø—Ä–æ—Å)."""
    image = Image.open(io.BytesIO(image_bytes)).convert("RGB")
    sub_images = dynamic_preprocess(image, image_size=1300, use_thumbnail=True)

    # –§–æ—Ä–º–∏—Ä—É–µ–º prompt
    content = [{"type": "image", "image": img} for img in sub_images]
    content.append({
    "type": "text",
    "text": f"""
You are an expert assistant for solving school-level math and physics diagram questions.
Your task is to analyze the given image(s) and determine which statements (A‚ÄìF) are TRUE.

Each question contains several statements labeled with A‚ÄìF.
One or more of them may be correct.

Guidelines:
- Carefully examine geometric or physical relations in the image.
- Ignore any text outside the image.
- Think briefly and logically, but don't think too long.
- The FINAL ANSWER must be **only** the correct capital letters (A‚ÄìF) without any spaces, commas, or extra words.
- If none are correct, answer "A" by default.
- Example of valid final line: BD

Question:
{question_text}

Answer:
"""
})

    

    messages = [{"role": "user", "content": content}]
    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

    inputs = processor(
        text=[text],
        images=sub_images,
        padding=True,
        return_tensors="pt"
    ).to(DEVICE)

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
    with torch.no_grad():
        generated_ids = model.generate(
            **inputs,
            max_new_tokens=3500,
            do_sample=False,
            temperature=0.0,
            top_k=1,
            pad_token_id=processor.tokenizer.eos_token_id,
            repetition_penalty=1.2
        )

    input_length = inputs.input_ids.shape[1]
    generated_ids = generated_ids[:, input_length:]
    response = processor.batch_decode(
        generated_ids,
        skip_special_tokens=True,
        clean_up_tokenization_spaces=False
    )[0].strip()

    answer = extract_answer_from_response(response)
    if not answer:
        answer = "AB"  # fallback, —á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ –ø—É—Å—Ç–æ–≥–æ –æ—Ç–≤–µ—Ç–∞

    # üîπ –õ–û–ì–ò–†–û–í–ê–ù–ò–ï
    print(f"{'='*80}")
    print(f"‚ùì –í–û–ü–†–û–°:\n{question_text}")
    print(f"{'-'*80}")
    print(f"ü§ñ –ü–û–õ–ù–´–ô –û–¢–í–ï–¢ –ú–û–î–ï–õ–ò:\n{response}")
    print(f"{'-'*80}")
    print(f"‚úÖ –ò–ó–í–õ–ï–ß–ï–ù–ù–´–ô –û–¢–í–ï–¢: {answer}")
    print(f"{'='*80}\n")

    return answer


  

# =============================================================================
# üîπ –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è (—á—Ç–µ–Ω–∏–µ input.pickle –∏ –∑–∞–ø–∏—Å—å output.json)
# =============================================================================

def main():
    input_path = "input.pickle"
    output_path = "output.json"

    # –ß–∏—Ç–∞–µ–º –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    with open(input_path, "rb") as f:
        data = pickle.load(f)

    results = []
    for item in data:
        rid = item.get("rid")
        question = item.get("question", "")
        image_bytes = item.get("image", None)

        if image_bytes is None:
            print(f"‚ö†Ô∏è RID {rid} has no image, skipping.")
            continue

        answer = infer_one(model, processor, image_bytes, question)
        results.append({"rid": rid, "answer": answer})

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print(f"\n‚úÖ Done. Results saved to {output_path}")

if __name__ == "__main__":
    main()
